{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the device\n",
    "\n",
    "Here we check if cuda is actually available on the device.\n",
    "The will check hardware and software support. \\\n",
    "Pytorch can also be installed without cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "# import pytorch library\n",
    "import torch\n",
    "\n",
    "# Check if cuda is actually available on the device.\n",
    "# The will check hardware and software support.\n",
    "# Pytorch can also be installed without cuda.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # Use the gpu\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # Use the cpu\n",
    "    \n",
    "print(\"using\", device, \"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "This is more or less a replica of the code we have used in the c++ example. It uses matrix multiplication instead of addition because the implementation of matrix addition in pytorch is way faster than what we did. As you can see it is much easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating random matrices: 2.687039375305176 seconds\n",
      "Host speed: 17.814164876937866 seconds (cpu)\n",
      "Device Speed with overhead: 0.887509822845459 seconds (cuda:0)\n",
      "Device Speed: 0.5372822284698486 seconds (cuda:0)\n"
     ]
    }
   ],
   "source": [
    "# import our timer\n",
    "import time\n",
    "\n",
    "# Define our workload. You can play around with the matrix size to change the execution time.\n",
    "matrix_size = 32 * 512\n",
    "\n",
    "# Allocate matrices on the host.\n",
    "print(\"Creating random matrices: \", end=\"\")\n",
    "start = time.time()\n",
    "\n",
    "a = torch.randn(matrix_size, matrix_size)\n",
    "b = torch.randn(matrix_size, matrix_size)\n",
    "\n",
    "print(str(time.time() - start) + \" seconds\")\n",
    "\n",
    "# Measure the execution speed on the host.\n",
    "print(\"Host speed: \", end=\"\")\n",
    "start = time.time()\n",
    "\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(str(time.time() - start) + \" seconds (\" + str(c.device) + \")\")\n",
    "\n",
    "# Copy the actual data to device.\n",
    "print(\"Device Speed with overhead: \", end=\"\")\n",
    "start = time.time()\n",
    "\n",
    "a_gpu = a.to(device)\n",
    "b_gpu = b.to(device)\n",
    "\n",
    "# The execution speed on the device with memory copying overhead.\n",
    "# Note that pytorch automatically uses the cuda device when the\n",
    "# used variables have been copied to the device.\n",
    "c_gpu = torch.matmul(a_gpu,b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(str(time.time() - start) + \" seconds (\" + str(c_gpu.device) + \")\")\n",
    "\n",
    "# The raw execution speed on the device.\n",
    "print(\"Device Speed: \", end=\"\")\n",
    "start = time.time()\n",
    "\n",
    "torch.matmul(a_gpu,b_gpu)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(str(time.time() - start) + \" seconds (\" + str(c_gpu.device) + \")\")\n",
    "\n",
    "# Done ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
